{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28da59df-150a-4b36-8340-49205bdedebe",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "This PyTorch summary is mainly based on https://pytorch.org/tutorials/beginner/basics/intro.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51dfc9d-7ef5-4a36-b8a7-8e70b22bb228",
   "metadata": {},
   "source": [
    "## 1. Tensors\n",
    "---\n",
    "\n",
    "- very similar to arrays and matrices\n",
    "- used to encode inputs and outputs of a model, as well as the model's parameters\n",
    "- can run on GPUs\n",
    "- optimized for automatic differentiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3f6207-2bde-47e1-be67-3759f679aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb52cdd-000c-4a02-8ef6-bd52082f3025",
   "metadata": {},
   "source": [
    "#### Initializing a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756e1997-e8f6-4566-a7c7-cc8e31bc3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "# from numpy array \n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "# random or constant value\n",
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d670304-4dbd-480b-b56f-245834350261",
   "metadata": {},
   "source": [
    "#### Attributes of a Tensor \n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1188f3-5155-4679-b51f-97b9e391431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(shape)\n",
    "shape = tensor.shape # (3,4)\n",
    "type = tensor.dtype # torch.float32\n",
    "dev = tensor.device # type='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134067d-f9e9-4907-a999-09b9e1d836e9",
   "metadata": {},
   "source": [
    "#### Moving Tensors to GPU \n",
    "By default tensors are created on CPU. You must explicitly move tensors to GPU (after checking for GPU availability). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3825bec6-c1c3-4096-989e-6630c9ef2599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU resources are available\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    print(\"GPU resources are available\")\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40298592-d106-4264-a6cf-70d6e4e936bd",
   "metadata": {},
   "source": [
    "#### Operations on Tensors\n",
    "Operations include arithmetic, linear algebra, matrix multiplication, and many more. Each operation can be run on GPU. Most of the operations are really similar to `numpy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e509323-5553-4620-ad66-d2a5e0df4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing & Slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "first_row = tensor[0]\n",
    "first_col = tensor[:, 0]\n",
    "elem = tensor[0,0]\n",
    "\n",
    "# Arithmetic Operations\n",
    "y1 = tensor @ tensor.T # matrix multiplication\n",
    "y2 = tensor * tensor   # element-wise product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a3cf2-1090-42f9-8815-5898be35f173",
   "metadata": {},
   "source": [
    "## 2. Datasets & DataLoaders\n",
    "---\n",
    "\n",
    "- PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`\n",
    "- allow to use pre-loaded datasets as well as your own data\n",
    "- `Dataset` stores samples\n",
    "- `DataLoder` wraps an iterable around the `Dataset` to easily access samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71ff3e-6c84-4fa9-9773-cd2738e2cbd3",
   "metadata": {},
   "source": [
    "#### Loading external Dataset\n",
    "You can freely access pre-loaded datasets. For example, `FashionMNIST` is a dataset containing 60'000 training examples and 10'000 test examples. Each sample comprises a 28x28 grayscale image and an associated lablem from 10 classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c5e4e1-29f9-4e49-a4d7-c61eed436e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # root is path where the train/test data is stored\n",
    "    train=True, # specifies training or test dataset\n",
    "    download=True, # downloads the data from Internet \n",
    "    transform=ToTensor() # specify the feature and label transformation\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f2ee1-b56c-49e7-adaa-094e9ffda3c1",
   "metadata": {},
   "source": [
    "#### Iterating over Dataset & Visualization\n",
    "You can index into the dataset using `training_data[index]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf9e99d6-4935-46b0-865d-47e23f3fd600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "img, label = training_data[sample_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f4ce0-1b14-43a4-832c-3fcd9061afe5",
   "metadata": {},
   "source": [
    "### Preparing your data for training with DataLoaders\n",
    "While training, you typically pass samples in minibathces, and reshuffle the data in every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b5e9b5-a84c-4378-a4ba-5fa72c9466c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c255a8-060b-421e-8d81-1aaf513a587d",
   "metadata": {},
   "source": [
    "#### Iterate through DataLoader\n",
    "After having loaded the dataset into `DataLoader`, we can iterate through the dataset. Each iteration returns a batch of `train_features` and `train_labels` containing 64 features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7061141d-9648-4bac-a968-80a03fc406de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(train_features.size())\n",
    "print(train_labels.size())\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8fdb2-425f-4b1f-a5c5-b12597515234",
   "metadata": {},
   "source": [
    "## 3. Neural Networks\n",
    "---\n",
    "\n",
    "`torch.nn` provides all the building blocks required to build your neural network. <br> Really useful resource: \n",
    "- https://pytorch.org/docs/stable/nn.html\n",
    "- tuning hyperparameters: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6719c44d-0c89-4656-8dd3-17aae0bd3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from torch import nn \n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3887b0-8876-4178-80bf-9ae850e38d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b2149-7101-422b-b3c5-32a8cc25eff8",
   "metadata": {},
   "source": [
    "Define your own neural network by subclassing `nn.Module`, and initialize the layers in `__init__`. You must implement the operations on input data in the `forward` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54d9939-61ca-47b6-a81a-62389e877fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeuralNetworks class inherits from nn.Module \n",
    "class NeuralNetwork(nn.Module): \n",
    "    # constructor \n",
    "    # self refers to the instance of the class that is being created \n",
    "    # alsows object to access its attributes & methods \n",
    "    def __init__(self):\n",
    "        # call constructor of nn.Module class \n",
    "        super().__init__() \n",
    "        # this layer flattens the input \n",
    "        # multi-dimensional input is transformed into 1D vector \n",
    "        self.flatten = nn.Flatten() \n",
    "        # define intermediate layers and activation functions\n",
    "        self.linear_relu_stack = nn.Sequential( \n",
    "            nn.Linear(28 * 28, 512), # input layer: 784 to 512 units\n",
    "            nn.ReLU(),               # ReLu activation\n",
    "            nn.Linear(512, 512),     # hidden layer: 512 to 512 units\n",
    "            nn.ReLU(),               # ReLu activation\n",
    "            nn.Linear(512, 10)       # output layer: 512 to 10 units (for classification)\n",
    "        )\n",
    "    # defines how input x flows through layers \n",
    "    # this function is called automatically, don't call it yourself! \n",
    "    def forward(self, x): \n",
    "        # input is always flattened before being passed to next layer \n",
    "        x = self.flatten(x)\n",
    "        # flattened input is passed though sequential stack of linear and ReLU layers\n",
    "        # output is tensor called logits\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # output is typically passed then to loss function or activation function\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a4d5e-2ac0-4951-b8da-9c068c7c84b8",
   "metadata": {},
   "source": [
    "Now, create an instance of `NeuralNetwork`, and move it to your GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed943cc3-340c-4e9c-83e8-4ba82fb6322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62884e7a-240b-4581-9986-1fb8566b2792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probabilities: tensor([[0.1101, 0.1107, 0.0985, 0.0929, 0.0946, 0.0971, 0.0998, 0.0999, 0.0987,\n",
      "         0.0976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Predicted class: tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(\"Class probabilities:\", pred_probab)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(\"Predicted class:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541fa5d-f884-4c44-beac-3ffa3772b21f",
   "metadata": {},
   "source": [
    "#### Going into Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2b2ba8-6e35-4c5a-acb7-d91de1fc1b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input batch of 3 images of size 28x28\n",
    "input_image = torch.rand(3, 28, 28) \n",
    "input_image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94156e98-3249-4a04-8d6c-76f0705a6b42",
   "metadata": {},
   "source": [
    "#### nn.Flatten\n",
    "We convert our three 2-dimensional 28x28 images into three 1-dimensional 784 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f881d1-c5e2-47b5-add7-1dc0676d6446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "flat_image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa108b-172b-40eb-9f44-4126f1c037f8",
   "metadata": {},
   "source": [
    "#### nn.Linear\n",
    "Apply linear transformation on the input using its stored weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e07b09f6-730f-4d8c-a6a6-be49185e13b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "hidden1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52132e-a673-4da7-b1c0-0396957319e7",
   "metadata": {},
   "source": [
    "#### nn.ReLU\n",
    "Introduce nonlinearity, helping neural networks to learn complex relations. You can observe that negative values are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403f61ce-f56e-4712-ac28-16f5338f855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.3229,  0.1037, -0.2146, -0.1409,  0.2887, -0.2952, -0.0498, -0.7032,\n",
      "          0.1779, -0.1321,  0.0843,  0.0864, -0.3566,  0.4613,  0.3330, -0.4496,\n",
      "          0.5064, -0.3268, -0.2587, -0.3868],\n",
      "        [ 0.4221,  0.3423, -0.1939, -0.0010,  0.2715, -0.0034, -0.3342, -0.6018,\n",
      "          0.2680, -0.0037,  0.0637,  0.0131,  0.1864,  0.1821,  0.4224, -0.6231,\n",
      "          0.1404, -0.3224, -0.6558, -0.4040],\n",
      "        [ 0.1256,  0.1455, -0.0943, -0.3716,  0.1173, -0.1546, -0.2580, -0.1659,\n",
      "          0.3310, -0.0741, -0.2854,  0.0939,  0.2930,  0.1903,  0.3121, -0.2703,\n",
      "          0.2233, -0.4708, -0.7156, -0.2854]], grad_fn=<AddmmBackward0>)\n",
      "After ReLU: tensor([[0.3229, 0.1037, 0.0000, 0.0000, 0.2887, 0.0000, 0.0000, 0.0000, 0.1779,\n",
      "         0.0000, 0.0843, 0.0864, 0.0000, 0.4613, 0.3330, 0.0000, 0.5064, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.4221, 0.3423, 0.0000, 0.0000, 0.2715, 0.0000, 0.0000, 0.0000, 0.2680,\n",
      "         0.0000, 0.0637, 0.0131, 0.1864, 0.1821, 0.4224, 0.0000, 0.1404, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.1256, 0.1455, 0.0000, 0.0000, 0.1173, 0.0000, 0.0000, 0.0000, 0.3310,\n",
      "         0.0000, 0.0000, 0.0939, 0.2930, 0.1903, 0.3121, 0.0000, 0.2233, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before ReLU:\", hidden1)\n",
    "relu = nn.ReLU()\n",
    "hidden1 = relu(hidden1)\n",
    "print(\"After ReLU:\", hidden1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653ae9c-af8a-4bee-b415-d8032c649716",
   "metadata": {},
   "source": [
    "#### Model Parameters \n",
    "Recall that all the layers within your neural network are parameterized, i.e. have weights that are optimized during training. You can access the parameters using `parameters()` or `named_parameters()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e7e071f-e5f1-4755-8eff-aa4c5da537e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0243,  0.0167, -0.0194,  ...,  0.0296, -0.0082,  0.0232],\n",
      "        [-0.0179,  0.0143, -0.0002,  ...,  0.0284, -0.0255,  0.0004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0239, -0.0044], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0146,  0.0002, -0.0413,  ..., -0.0398, -0.0345, -0.0075],\n",
      "        [ 0.0190, -0.0132, -0.0306,  ..., -0.0389, -0.0376, -0.0229]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0318, -0.0055], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0204, -0.0272,  0.0435,  ...,  0.0345,  0.0009,  0.0365],\n",
      "        [ 0.0027,  0.0173, -0.0189,  ...,  0.0260, -0.0023,  0.0058]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0379, 0.0325], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Structure:\", model)\n",
    "\n",
    "for name, parameter in model.named_parameters(): \n",
    "    print(f\"Layer: {name} | Size: {parameter.size()} | Values : {parameter[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee156a-0f11-4520-89c9-745b4a275199",
   "metadata": {},
   "source": [
    "## Training Neural Networks using Automatic Differentiation\n",
    "---\n",
    "In back propagation, the parameters are adjusted according to the gradient of the loss function wrt. to the given parameter. To compute those gradients, PyTorch provides a built-in differentiation enginve called `torch.autograd`. \n",
    "\n",
    "Consider the simplest neural network with one-layer. We must compute the gradient with wrt. $w$ and $b$. We do so by setting `requires_grad=True` to indicate that we must differentiate the loss function wrt. to those parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a84a5d4-2c41-451b-bbd9-6aeede70c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True) # important\n",
    "b = torch.randn(3, requires_grad=True)    # important\n",
    "z = torch.matmul(x, w) + b \n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93988ec3-1be8-4536-bc35-efa2f0c8da39",
   "metadata": {},
   "source": [
    "To compute those derivates under some fixed $x$ and $y$, call the function `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a35f90d4-adda-4f41-be9c-ac4c7fc6dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "w_grad = w.grad\n",
    "b_grad = b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ce11a-035f-42fa-bf4b-d51c93e3a1b3",
   "metadata": {},
   "source": [
    "## Optimizing Neural Network Parameters\n",
    "---\n",
    "\n",
    "Until now we have learned, how to build a neural network and how to compute the gradient in a single point. However, training a neural network is an iterative process, where in each iteration, the neural network makes a guess about the output, computes the error in its guess, collects the derivatives of the error wrt. to its parameters, and optimizes these parameters using gradient descent.  \n",
    "\n",
    "In the following, we build a neural network for multi-classification based on the `FashionMNIST` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eb33f18-b067-440e-a6bb-f29f4308ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1779bf-2f9a-4a46-893e-1673c39fca41",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "The following hyperparameters are important for training: \n",
    "- **Number of Epochs:** number of times to iterate over the dataset\n",
    "- **Batch Size:** number of data samples propagated through the network before the parameters are updated\n",
    "- **Learning Rate**: how much to update models parameters at each batch/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfc90a07-e214-4d41-adde-a5863f532c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1E-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c148a-b1e0-4f8c-b49a-9c98947d28d4",
   "metadata": {},
   "source": [
    "#### Optimization Loop\n",
    "Train and optimize the neural network with an optimization loop. Each iteration of the optimization loop is called an **epoch**. Each epoch consists of two parts: \n",
    "- **Training Loop:** iterate over the training data and try to converge to optimal parameters\n",
    "- **Validation:** iterate over test data to check if model performance is improving\n",
    "\n",
    "Optimization inside the loop happens in three steps: \n",
    "- Call `optimizer.zero_grad()` to reset the gradients of model parameters at each iteration. This is done since gradients are added up by default.\n",
    "- Backpropagate the prediction loss with a call to `loss.backward()`\n",
    "- Once the gradient is computed, call `optimizer.step()` to update the parameters\n",
    "\n",
    "#### Loss Function\n",
    "Commonly used loss functions include `nn.MSELoss` (mean squared error) for regression, and `nn.NLLoss` (negative log likelihood) for classification. `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`. \n",
    "```python\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "#### Optimizer\n",
    "A commonly used optimization algorithm is Stochastic Gradient Descent. You can find more optimizers here: https://pytorch.org/docs/stable/optim.html <br>\n",
    "Initialize the optimizer by registering the model's parameters that need to be trained (weights and bias), and passing in the learning rate. \n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "#### Full Implementation of Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "802c72a0-a4fb-4084-b3e7-e58243a3ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer): \n",
    "    \n",
    "    # total number of samples in dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # puts model into training mode \n",
    "    model.train()\n",
    "    \n",
    "    # iterate over each batch of data \n",
    "    # batch = index of current batch\n",
    "    # (X, y) = input features and corresponding features \n",
    "    for batch, (X, y) in enumerate(dataloader): \n",
    "        # moving to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # model generates predictions based on input batch X \n",
    "        pred = model(X)  \n",
    "        # prediction error \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # performs backpropagation, calculating gradient of loss wrt parameters \n",
    "        loss.backward()\n",
    "        # updates model parameters using computed gradient \n",
    "        optimizer.step()\n",
    "        # reset gradient for next iteration since they accumulate \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # every 100 batches, print current loss\n",
    "        if batch % 100 == 0: \n",
    "            # converts loss tensor into float \n",
    "            loss = loss.item() \n",
    "            # how many samples have been processed so far\n",
    "            current = batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02ee59bd-d1aa-42b4-b0fa-de28df77bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    # puts model in evaluation mode \n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0 \n",
    "\n",
    "    # disables gradient calculation to ensure that no parameters are updated \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # loop through batches \n",
    "        for X, y in dataloader:\n",
    "            # moving to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # model generates predictions based on input batch X \n",
    "            pred = model(X)\n",
    "            # update loss \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # count correct predictions (classification)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "197c1fb7-ce9c-4c07-88be-91bc17bb1f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305279  [   64/60000]\n",
      "loss: 2.300761  [ 6464/60000]\n",
      "loss: 2.279824  [12864/60000]\n",
      "loss: 2.275269  [19264/60000]\n",
      "loss: 2.262370  [25664/60000]\n",
      "loss: 2.231902  [32064/60000]\n",
      "loss: 2.242961  [38464/60000]\n",
      "loss: 2.210954  [44864/60000]\n",
      "loss: 2.206213  [51264/60000]\n",
      "loss: 2.182332  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 2.173689 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.176937  [   64/60000]\n",
      "loss: 2.180639  [ 6464/60000]\n",
      "loss: 2.121284  [12864/60000]\n",
      "loss: 2.142006  [19264/60000]\n",
      "loss: 2.104252  [25664/60000]\n",
      "loss: 2.037495  [32064/60000]\n",
      "loss: 2.072766  [38464/60000]\n",
      "loss: 1.999086  [44864/60000]\n",
      "loss: 1.994117  [51264/60000]\n",
      "loss: 1.939277  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.929430 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.946602  [   64/60000]\n",
      "loss: 1.937617  [ 6464/60000]\n",
      "loss: 1.817015  [12864/60000]\n",
      "loss: 1.862474  [19264/60000]\n",
      "loss: 1.768032  [25664/60000]\n",
      "loss: 1.696554  [32064/60000]\n",
      "loss: 1.727879  [38464/60000]\n",
      "loss: 1.625832  [44864/60000]\n",
      "loss: 1.632037  [51264/60000]\n",
      "loss: 1.538884  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.547783 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.601099  [   64/60000]\n",
      "loss: 1.579491  [ 6464/60000]\n",
      "loss: 1.422680  [12864/60000]\n",
      "loss: 1.500982  [19264/60000]\n",
      "loss: 1.380642  [25664/60000]\n",
      "loss: 1.360151  [32064/60000]\n",
      "loss: 1.383811  [38464/60000]\n",
      "loss: 1.302764  [44864/60000]\n",
      "loss: 1.328254  [51264/60000]\n",
      "loss: 1.235782  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.257416 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.325715  [   64/60000]\n",
      "loss: 1.318929  [ 6464/60000]\n",
      "loss: 1.148890  [12864/60000]\n",
      "loss: 1.263700  [19264/60000]\n",
      "loss: 1.133168  [25664/60000]\n",
      "loss: 1.149308  [32064/60000]\n",
      "loss: 1.180317  [38464/60000]\n",
      "loss: 1.109803  [44864/60000]\n",
      "loss: 1.143972  [51264/60000]\n",
      "loss: 1.068081  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.084772 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.148510  [   64/60000]\n",
      "loss: 1.161378  [ 6464/60000]\n",
      "loss: 0.974844  [12864/60000]\n",
      "loss: 1.120410  [19264/60000]\n",
      "loss: 0.987208  [25664/60000]\n",
      "loss: 1.013533  [32064/60000]\n",
      "loss: 1.059052  [38464/60000]\n",
      "loss: 0.991273  [44864/60000]\n",
      "loss: 1.028392  [51264/60000]\n",
      "loss: 0.965989  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.976487 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.029463  [   64/60000]\n",
      "loss: 1.062121  [ 6464/60000]\n",
      "loss: 0.858416  [12864/60000]\n",
      "loss: 1.025947  [19264/60000]\n",
      "loss: 0.897724  [25664/60000]\n",
      "loss: 0.919747  [32064/60000]\n",
      "loss: 0.980265  [38464/60000]\n",
      "loss: 0.914972  [44864/60000]\n",
      "loss: 0.949492  [51264/60000]\n",
      "loss: 0.897418  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.903201 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.942444  [   64/60000]\n",
      "loss: 0.993383  [ 6464/60000]\n",
      "loss: 0.775657  [12864/60000]\n",
      "loss: 0.958845  [19264/60000]\n",
      "loss: 0.838400  [25664/60000]\n",
      "loss: 0.850878  [32064/60000]\n",
      "loss: 0.924474  [38464/60000]\n",
      "loss: 0.863569  [44864/60000]\n",
      "loss: 0.892294  [51264/60000]\n",
      "loss: 0.847318  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.850229 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.874835  [   64/60000]\n",
      "loss: 0.941524  [ 6464/60000]\n",
      "loss: 0.713486  [12864/60000]\n",
      "loss: 0.908307  [19264/60000]\n",
      "loss: 0.796171  [25664/60000]\n",
      "loss: 0.798782  [32064/60000]\n",
      "loss: 0.881663  [38464/60000]\n",
      "loss: 0.827203  [44864/60000]\n",
      "loss: 0.849052  [51264/60000]\n",
      "loss: 0.808500  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.809893 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.820084  [   64/60000]\n",
      "loss: 0.899652  [ 6464/60000]\n",
      "loss: 0.664981  [12864/60000]\n",
      "loss: 0.869429  [19264/60000]\n",
      "loss: 0.764353  [25664/60000]\n",
      "loss: 0.758579  [32064/60000]\n",
      "loss: 0.846790  [38464/60000]\n",
      "loss: 0.800429  [44864/60000]\n",
      "loss: 0.815277  [51264/60000]\n",
      "loss: 0.777210  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.777819 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
